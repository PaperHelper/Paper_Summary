{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "캡스톤디자인_논문요약.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSDPFT8mepsg",
        "outputId": "141bcfc7-0404-4258-bcd9-14e7f9110436"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltc5Rz7CliLj",
        "outputId": "80328bc7-795b-4108-9a08-c95532a810d7"
      },
      "source": [
        "!pip install pdfminer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/a3/155c5cde5f9c0b1069043b2946a93f54a41fd72cc19c6c100f6f2f5bdc15/pdfminer-20191125.tar.gz (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 20.3MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/16/9627ab0493894a11c68e46000dbcc82f578c8ff06bc2980dcd016aea9bd3/pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 56.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-cp37-none-any.whl size=6140091 sha256=fa08eb72973fcd041abb81c8a0f16f7dcc8c904293b680dd3ae6d4c674fe8d8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/00/af/720a55d74ba3615bb4709a3ded6dd71dc5370a586a0ff6f326\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8YiawJ9JJUm"
      },
      "source": [
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEUlEHIMJqm9",
        "outputId": "c0ec98e5-8a45-416f-ab84-bb8ed214dab5"
      },
      "source": [
        "from io import StringIO\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "# %load_ext google.colab.data_table\n",
        "\n",
        "def pdf_to_text(filename):\n",
        "  output_string = StringIO()\n",
        "  with open(filename, 'rb') as in_file:\n",
        "      parser = PDFParser(in_file)\n",
        "      doc = PDFDocument(parser)\n",
        "      rsrcmgr = PDFResourceManager()\n",
        "      device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "      interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "      for page in PDFPage.create_pages(doc):\n",
        "          interpreter.process_page(page)\n",
        "  return output_string.getvalue()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9TsIA-UJxNv",
        "outputId": "121763c6-c48d-41eb-fb88-ddc22ccbaffa"
      },
      "source": [
        "def preprocess_text(filename):\n",
        "  text = pdf_to_text(filename)\n",
        "  text = text.partition('Introduction')[2]\n",
        "  if 'Acknowledgements' in text:\n",
        "    text = text.partition('Acknowledgements')[0].strip()\n",
        "  else:\n",
        "    text = text.partition('References')[0].strip()\n",
        "  # print(text)\n",
        "  text = re.sub('ﬀ','ff',text)\n",
        "  text = re.sub('[(].+[)]','',text)\n",
        "  text = re.sub('[[][^a-zA-Z]+[]]','',text)\n",
        "  text = re.sub('-\\n','',text)\n",
        "  text = re.sub('\\n',' ',text)\n",
        "  paragraphs = re.split('\\s{2,}',text)\n",
        "  \"\"\"for p in paragraphs:\n",
        "    print(p+'\\n')\"\"\"\n",
        "  new_paragraphs = [p for p in paragraphs if re.search('[a-z]+',p,re.I)]\n",
        "\n",
        "  paragraphs = []\n",
        "  new_p = \"\"\n",
        "  for p in new_paragraphs:\n",
        "    if len(new_p) != 0:\n",
        "      new_p += ' '+p\n",
        "      if p.endswith('.') and re.search('al\\s*.$',p) == None:\n",
        "        paragraphs.append(new_p)\n",
        "        new_p = \"\"\n",
        "    else:\n",
        "      if p.endswith('.') and re.search('al\\s*.$',p) == None:\n",
        "        paragraphs.append(p)\n",
        "      else:\n",
        "        new_p = p\n",
        "\n",
        "  new_paragraphs = [p for p in paragraphs if re.search('Table.+:',p) == None and re.search('Fig.+:',p) == None]\n",
        "\n",
        "  return new_paragraphs"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by deﬁning surrogate regression and classiﬁcation problems on a large set of proposals , anchors , or window centers . Their performances are signiﬁcantly inﬂuenced by postprocessing steps to collapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors . To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. This end-to-end philosophy has led to signiﬁcant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on challenging benchmarks. This paper aims to bridge this gap.\n",
            "\n",
            "We streamline the training pipeline by viewing object detection as a direct set prediction problem. We adopt an encoder-decoder architecture based on transformers , a popular architecture for sequence prediction. The self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence, make these architectures particularly suitable for speciﬁc constraints of set prediction such as removing duplicate predictions.\n",
            "\n",
            "Our DEtection TRansformer predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. DETR simpliﬁes the detection pipeline by dropping multiple hand-designed components that encode prior knowledge, like spatial anchors or non-maximal suppression. Unlike most existing detection methods, DETR doesn’t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes.1.\n",
            "\n",
            "Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with parallel decoding . In contrast, previous work focused on autoregressive decoding with RNNs . Our matching loss function uniquely assigns a prediction to a ground truth object, and is invariant to a permutation of predicted objects, so we can emit them in parallel. We evaluate DETR on one of the most popular object detection datasets, COCO , against a very competitive Faster R-CNN baseline . Faster RCNN has undergone many design iterations and its performance was greatly improved since the original publication. Our experiments show that our new model achieves comparable performances. More precisely, DETR demonstrates signiﬁcantly better performance on large objects, a result likely enabled by the non-local computations of the transformer. It obtains, however, lower performances on small objects. We expect that future work will improve this aspect in the same way the development of FPN did for Faster R-CNN.\n",
            "\n",
            "Training settings for DETR differ from standard object detectors in multiple ways. The new model requires extra-long training schedule and beneﬁts 1 In our work we use standard implementations of Transformers and ResNet backbones from standard deep learning libraries.\n",
            "\n",
            "transformer encoder-decoderCNNset of box predictionsbipartite matching lossno object set of image features\fEnd-to-End Object Detection with Transformers from auxiliary decoding losses in the transformer. We thoroughly explore what components are crucial for the demonstrated performance.\n",
            "\n",
            "The design ethos of DETR easily extend to more complex tasks. In our experiments, we show that a simple segmentation head trained on top of a pretrained DETR outperfoms competitive baselines on Panoptic Segmentation , a challenging pixel-level recognition task that has recently gained popularity.\n",
            "\n",
            "2 Related work Our work build on prior work in several domains: bipartite matching losses for set prediction, encoder-decoder architectures based on the transformer, parallel decoding, and object detection methods.\n",
            "\n",
            "2.1 Set Prediction There is no canonical deep learning model to directly predict sets. The basic set prediction task is multilabel classiﬁcation (see e.g., for references in the context of computer vision) for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements . The ﬁrst diﬃculty in these tasks is to avoid near-duplicates. Most current detectors use postprocessings such as non-maximal suppression to address this issue, but direct set prediction are postprocessing-free. They need global inference schemes that model interactions between all predicted elements to avoid redundancy. For constant-size set prediction, dense fully connected networks are suﬃcient but costly. A general approach is to use auto-regressive sequence models such as recurrent neural networks . In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian algorithm , to ﬁnd a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match. We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding, which we describe below.\n",
            "\n",
            "2.2 Transformers and Parallel Decoding Transformers were introduced by Vaswani et al . as a new attention-based building block for machine translation. Attention mechanisms are neural network layers that aggregate information from the entire input sequence. Transformers introduced self-attention layers, which, similarly to Non-Local Neural Networks , scan through each element of a sequence and update it by aggregating information from the whole sequence. One of the main advantages of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers are now Carion et al. replacing RNNs in many problems in natural language processing, speech processing and computer vision .\n",
            "\n",
            "Transformers were ﬁrst used in auto-regressive models, following early sequence to-sequence models , generating output tokens one by one. However, the prohibitive inference cost lead to the development of parallel sequence generation, in the domains of audio , machine translation , word representation learning , and more recently speech recognition . We also combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.\n",
            "\n",
            "2.3 Object detection Most modern object detection methods make predictions relative to some initial guesses. Two-stage detectors predict boxes w.r.t. proposals, whereas single-stage methods make predictions w.r.t. anchors or a grid of possible object centers . Recent work demonstrate that the ﬁnal performance of these systems heavily depends on the exact way these initial guesses are set. In our model we are able to remove this hand-crafted process and streamline the detection process by directly predicting the set of detections with absolute box prediction w.r.t. the input image rather than an anchor.\n",
            "\n",
            "Set-based loss. Several object detectors used the bipartite matching loss. However, in these early deep learning models, the relation between different prediction was modeled with convolutional or fully-connected layers only and a hand-designed NMS post-processing can improve their performance. More recent detectors use non-unique assignment rules between ground truth and predictions together with an NMS.\n",
            "\n",
            "Learnable NMS methods and relation networks explicitly model relations between different predictions with attention. Using direct set losses, they do not require any post-processing steps. However, these methods employ additional hand-crafted context features like proposal box coordinates to model relations between detections eﬃciently, while we look for solutions that reduce the prior knowledge encoded in the model.\n",
            "\n",
            "Recurrent detectors. Closest to our approach are end-to-end set predictions for object detection and instance segmentation . Similarly to us, they use bipartite-matching losses with encoder-decoder architectures based on CNN activations to directly produce a set of bounding boxes. These approaches, however, were only evaluated on small datasets and not against modern baselines. In particular, they are based on autoregressive models , so they do not leverage the recent transformers with parallel decoding.\n",
            "\n",
            "3 The DETR model Two ingredients are essential for direct set predictions in detection: a set prediction loss that forces unique matching between predicted and ground truth End-to-End Object Detection with Transformers boxes; a set of objects and models their relation. We describe our architecture in detail in Figure 2.\n",
            "\n",
            "3.1 Object detection set prediction loss DETR infers a ﬁxed-size set of N predictions, in a single pass through the decoder, where N is set to be signiﬁcantly larger than the typical number of objects in an image. One of the main diﬃculties of training is to score predicted objects with respect to the ground truth. Our loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-speciﬁc losses.\n",
            "\n",
            "Let us denote by y the ground truth set of objects, and ˆy = {ˆyi}N i=1 the set of N predictions. Assuming N is larger than the number of objects in the image, we consider y also as a set of size N padded with ∅ . To ﬁnd a bipartite matching between these two sets we search for a permutation of N elements σ ∈ SN with the lowest cost: N i ˆσ = arg min σ∈SN Lmatch, where Lmatch is a pair-wise matching cost between ground truth yi and a prediction with index σ. This optimal assignment is computed eﬃciently with the Hungarian algorithm, following prior work .\n",
            "\n",
            "The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. Each element i of the ground truth set can be seen as a yi = where ci is the target class label (which may be ∅) and bi ∈ 4 is a vector that deﬁnes ground truth box center coordinates and its height and width relative to the image size. For the prediction with index σ and the predicted box as ˆbσ as −1{ci.\n",
            "\n",
            "This procedure of ﬁnding matching plays the same role as the heuristic assignment rules used to match proposal or anchors to ground truth objects in modern detectors. The main difference is that we need to ﬁnd one-to-one matching for direct set prediction without duplicates.\n",
            "\n",
            "The second step is to compute the loss function, the Hungarian loss for all pairs matched in the previous step. We deﬁne the loss similarly to the losses of common object detectors, i.e. a linear combination of a negative log-likelihood for class prediction and a box loss deﬁned later: N LHungarian = i=1 where ˆσ is the optimal assignment computed in the ﬁrst step . In practice, we down-weight the log-probability term when ci = ∅ by a factor 10 to account for Carion et al. class imbalance. This is analogous to how Faster R-CNN training procedure balances positive/negative proposals by subsampling . Notice that the matching cost between an object and ∅ doesn’t depend on the prediction, which means that in that case the cost is a constant. In the matching cost we use probabilities ˆpˆσ instead of log-probabilities. This makes the class prediction term commensurable to Lbox, and we observed better empirical performances.\n",
            "\n",
            "Bounding box loss. The second part of the matching cost and the Hungarian loss is Lbox that scores the bounding boxes. Unlike many detectors that do box predictions as a ∆ w.r.t. some initial guesses, we make box predictions directly. While such approach simplify the implementation it poses an issue with relative scaling of the loss. The most commonly-used 1 loss will have different scales for small and large boxes even if their relative errors are similar. To mitigate this issue we use a linear combination of the 1 loss and the generalized IoU loss Liou deﬁned as λiouLiou||1 where λiou, λL1 ∈ R are hyperparameters. These two losses are normalized by the number of objects inside the batch.\n",
            "\n",
            "32 , W0 32 .\n",
            "\n",
            "2 The input images are batched together, applying 0-padding adequately to ensure they all have the same dimensions as the largest image of the batch.\n",
            "\n",
            "Transformer decoder. The decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed self- and encoder-decoder attention mechanisms. The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while Vaswani et al. use an autoregressive model that predicts the output sequence one element at a time. We refer the reader unfamiliar with the concepts to the supplementary material. Since the decoder is also permutation-invariant, the N input embeddings must be different to produce different results. These input embeddings are learnt positional encodings that we refer to as object queries, and similarly to the encoder, we add them to the input of each attention layer. The N object queries are transformed into an output embedding by the decoder. They are then independently decoded into box coordinates and class labels by a feed forward network , resulting N ﬁnal predictions. Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.\n",
            "\n",
            "Prediction feed-forward networks . The ﬁnal prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function. Since we predict a ﬁxed-size set of N bounding boxes, where N is usually much larger than the actual number of objects of interest in an image, an additional special class label ∅ is used to represent that no object is detected within a slot. This class plays a similar role to the “background” class in the standard object detection approaches.\n",
            "\n",
            "Auxiliary decoding losses. We found helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number CNNset of image featurestransformer encoder……positional encoding+transformer decoderclass,boxclass,boxno objectno objectFFNFFNFFNFFNobject queriesbackboneencoderdecoderprediction heads\f8 Carion et al. of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers.\n",
            "\n",
            "4 Experiments We show that DETR achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO. Then, we provide a detailed ablation study of the architecture and loss, with insights and qualitative results. Finally, to show that DETR is a versatile and extensible model, we present results on panoptic segmentation, training only a small extension on a ﬁxed DETR model. We provide code and pretrained models to reproduce our experiments at https://github.com/facebookresearch/detr.\n",
            "\n",
            "Dataset. We perform experiments on COCO 2017 detection and panoptic segmentation datasets , containing 118k training images and 5k validation images. Each image is annotated with bounding boxes and panoptic segmentation. There are 7 instances per image on average, up to 63 instances in a single image in training set, ranging from small to large on the same images. If not speciﬁed, we report AP as bbox AP, the integral metric over multiple thresholds. For comparison with Faster R-CNN we report validation AP at the last training epoch, for ablations we report median over validation results from the last 10 epochs.\n",
            "\n",
            "Technical details. We train DETR with AdamW setting the initial transformer’s learning rate to 10−4, the backbone’s to 10−5, and weight decay to 10−4. All transformer weights are initialized with Xavier init , and the backbone is with ImageNet-pretrained ResNet model from torchvision with frozen batchnorm layers. We report results with two different backbones: a ResNet50 and a ResNet-101. The corresponding models are called respectively DETR and DETR-R101. Following , we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the ﬁrst convolution of this stage. The corresponding models are called respectively DETR-DC5 and DETR-DC5-R101 . This modiﬁcation increases the resolution by a factor of two, thus improving performance for small objects, at the cost of a 16x higher cost in the self-attentions of the encoder, leading to an overall 2x increase in computational cost. A full comparison of FLOPs of these models and Faster R-CNN is given in Table 1.\n",
            "\n",
            "Model GFLOPS/FPS #params AP AP50 AP75 APS APM APL Faster RCNN-DC5 Faster RCNN-FPN Faster RCNN-R101-FPN Faster RCNN-DC5+ Faster RCNN-FPN+ Faster RCNN-R101-FPN+ DETR DETR-DC5 DETR-R101 DETR-DC5-R101 166M 39.0 60.5 42.3 21.4 43.5 52.5 42M 40.2 61.0 43.8 24.2 43.5 52.0 60M 42.0 62.5 45.9 25.2 45.6 54.6 166M 41.1 61.4 44.3 22.9 45.9 55.0 42M 42.0 62.1 45.5 26.6 45.4 53.4 60M 44.0 63.9 47.8 27.2 48.1 56.0 41M 42.0 62.4 44.2 20.5 45.8 61.1 41M 43.3 63.1 45.9 22.5 47.3 61.1 60M 43.5 63.8 46.4 21.9 48.0 61.8 60M 44.9 64.7 47.7 23.7 49.5 62.3 time, some slots predict empty class. To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding conﬁdence. This improves AP by 2 points compared to ﬁltering out empty slots. Other training hyperparameters can be found in section A.4. For our ablation experiments we use training schedule of 300 epochs with a learning rate drop by a factor of 10 after 200 epochs, where a single epoch is a pass over all training images once. Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU . For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule.\n",
            "\n",
            "#layers GFLOPS/FPS #params 33.4M 37.4M 41.3M 49.2M AP AP50 APS APM APL with the 9x schedule and the described enhancements, which in total adds 1-2 AP. In the last section of Table 1 we show the results for multiple DETR models. To be comparable in the number of parameters we choose a model with 6 transformer and 6 decoder layers of width 256 with 8 attention heads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of which 23.5M are in ResNet-50, and 17.8M are in the transformer. Even though both Faster R-CNN and DETR are still likely to further improve with longer training, we can conclude that DETR can be competitive with Faster R-CNN with the same number of parameters, achieving 42 AP on the COCO val subset. The way DETR achieves this is by improving APL , however note that the model is still lagging behind in APS . DETR-DC5 with the same number of parameters and similar FLOP count has higher AP, but is still signiﬁcantly behind in APS too. Faster R-CNN and DETR with ResNet-101 backbone show comparable results as well.\n",
            "\n",
            "4.2 Ablations Attention mechanisms in the transformer decoder are the key components which model relations between feature representations of different detections. In our ablation analysis, we explore how other components of our architecture and loss inﬂuence the ﬁnal performance. For the study we choose ResNet-50-based DETR model with 6 encoder, 6 decoder layers and width 256. The model has 41.3M parameters, achieves 40.6 and 42.0 AP on short and long schedules respectively, and runs at 28 FPS, similarly to Faster R-CNN-FPN with the same backbone.\n",
            "\n",
            "Number of encoder layers. We evaluate the importance of global imagelevel self-attention by changing the number of encoder layers . Without encoder layers, overall AP drops by 3.9 points, with a more signiﬁcant drop of 6.0 AP on large objects. We hypothesize that, by using global scene reasoning, the encoder is important for disentangling objects. In Figure 3, we visualize the attention maps of the last encoder layer of a trained model, focusing on a few points in the image. The encoder seems to separate instances already, which likely simpliﬁes object extraction and localization for the decoder.\n",
            "\n",
            "dict objects out of the outputs of every decoder layer. We analyze the importance of each decoder layer by evaluating the objects that would be predicted at each stage of the decoding . Both AP and AP50 improve after every layer, totalling into a very signiﬁcant +8.2/9.5 AP improvement between the ﬁrst and the last layer. With its set-based loss, DETR does not need NMS by design. To verify this we run a standard NMS procedure with default parameters for the outputs after each decoder. NMS improves performance for the predictions from the ﬁrst decoder. This can be explained by the fact that a single decoding layer of the transformer is not able to compute any cross-correlations between the output elements, and thus it is prone to making multiple predictions for the same object. In the second and subsequent layers, the self-attention mechanism over the activations allows the model to inhibit duplicate predictions. We observe that the improvement brought by NMS diminishes as depth increases. At the last layers, we observe a small loss in AP as NMS incorrectly removes true positive predictions.\n",
            "\n",
            "Similarly to visualizing encoder attention, we visualize decoder attentions in Fig. 6, coloring attention maps for each predicted object in different colors. We observe that decoder attention is fairly local, meaning that it mostly attends to object extremities such as heads or legs. We hypothesise that after the encoder has separated instances via global attention, the decoder only needs to attend to the extremities to extract the class and object boundaries. Importance of FFN. FFN inside tranformers can be seen as 1 × 1 convolutional layers, making encoder similar to attention augmented convolutional networks . We attempt to remove it completely leaving only attention in the transformer layers. By reducing the number of network parameters from 41.3M to 28.7M, leaving only 10.8M in the transformer, performance drops by 2.3 AP, we thus conclude that FFN are important for achieving good results.\n",
            "\n",
            "ings . We experiment with various combinations of ﬁxed and learned encodings, results can be found in table 3. Output positional encodings are required and cannot be removed, so we experiment with either passing them once at decoder input or adding to queries at every decoder attention layer. In the ﬁrst experiment we completely remove spatial positional encodings and pass output positional encodings at input and, interestingly, the model still achieves more than 32 AP, losing 7.8 AP to the baseline. Then, we pass ﬁxed sine spatial positional encodings and the output encodings at input once, as in the original transformer , and ﬁnd that this leads to 1.4 AP drop compared to passing the positional encodings directly in attention. Learned spatial encodings passed to the attentions give similar results. Surprisingly, we ﬁnd that not passing any spatial encodings in the encoder only leads to a minor AP drop of 1.3 AP. When we pass the encodings to the attentions, they are shared across all layers, and the output encodings are always learned.\n",
            "\n",
            "Given these ablations, we conclude that transformer components: the global self-attention in encoder, FFN, multiple decoder layers, and positional encodings, all signiﬁcantly contribute to the ﬁnal object detection performance.\n",
            "\n",
            "spatial pos. enc.\n",
            "\n",
            "output pos. enc.\n",
            "\n",
            "encoder decoder decoder AP AP50 none sine at input learned at attn. none sine at attn.\n",
            "\n",
            "none sine at input learned at attn. sine at attn. sine at attn.\n",
            "\n",
            "learned at input learned at input learned at attn. learned at attn. learned at attn.\n",
            "\n",
            "simple ablations of different losses , but other means of combining them may achieve different results.\n",
            "\n",
            "4.3 Analysis Decoder output slot analysis In Fig. 7 we visualize the boxes predicted by different slots for all images in COCO 2017 val set. DETR learns different specialization for each query slot. We observe that each slot has several modes of operation focusing on different areas and box sizes. In particular, all slots have the mode for predicting image-wide boxes (visible as the red dots aligned in the middle of the plot). We hypothesize that this is related to the distribution of objects in COCO.\n",
            "\n",
            "Generalization to unseen numbers of instances. Some classes in COCO are not well represented with many instances of the same class in the same image. For example, there is no image with more than 13 giraffes in the training set. We create a synthetic image3 to verify the generalization ability of DETR . Our model is able to ﬁnd all 24 giraffes on the image which is clearly out of distribution. This experiment conﬁrms that there is no strong class-specialization in each object query.\n",
            "\n",
            "in a uniﬁed way. We perform our experiments on the panoptic annotations of the COCO dataset that has 53 stuff categories in addition to 80 things categories.\n",
            "\n",
            "We train DETR to predict boxes around both stuff and things classes on COCO, using the same recipe. Predicting boxes is required for the training to be possible, since the Hungarian matching is computed using distances between boxes. We also add a mask head which predicts a binary mask for each of the predicted boxes, see Figure 8. It takes as input the output of transformer decoder for each object and computes multi-head attention scores of this embedding over the output of the encoder, generating M attention heatmaps per object in a small resolution. To make the ﬁnal prediction and increase the resolution, an FPN-like architecture is used. We describe the architecture in more details in the supplement. The ﬁnal resolution of the masks has stride 4 and each mask is supervised independently using the DICE/F-1 loss and Focal loss .\n",
            "\n",
            "The mask head can be trained either jointly, or in a two steps process, where we train DETR for boxes only, then freeze all the weights and train only the mask head for 25 epochs. Experimentally, these two approaches give similar results, we report results using the latter method since it results in a shorter total wall-clock time training.\n",
            "\n",
            "Model Backbone PQ SQ RQ PQth SQth RQth PQst SQst RQst AP PanopticFPN++ UPSnet UPSnet-M PanopticFPN++ DETR DETR-DC5 DETR-R101 R50 R50 R50 R101 R50 R50 R101 To predict the ﬁnal panoptic segmentation we simply use an argmax over the mask scores at each pixel, and assign the corresponding categories to the resulting masks. This procedure guarantees that the ﬁnal masks have no overlaps and, therefore, DETR does not require a heuristic that is often used to align different masks.\n",
            "\n",
            "Training details. We train DETR, DETR-DC5 and DETR-R101 models following the recipe for bounding box detection to predict boxes around stuff and things classes in COCO dataset. The new mask head is trained for 25 epochs . During inference we ﬁrst ﬁlter out the detection with a conﬁdence below 85%, then compute the per-pixel argmax to determine in which mask each pixel belongs. We then collapse different mask predictions of the same stuff category in one, and ﬁlter the empty ones .\n",
            "\n",
            "Main results. Qualitative results are shown in Figure 9. In table 5 we compare our uniﬁed panoptic segmenation approach with several established methods that treat things and stuff differently. We report the Panoptic Quality and the break-down on things . We also report the mask AP , before any panoptic post-treatment (in our case, before taking the pixel-wise argmax). We show that DETR outperforms published results on COCO-val 2017, as well as our strong PanopticFPN baseline . The result break-down shows that DETR is especially dominant on stuff classes, and we hypothesize that the global reasoning allowed by the encoder attention is the key element to this result. For things class, despite a severe deﬁcit of up to 8 mAP compared to the baselines on the mask AP computation, DETR obtains competitive PQth. We also evaluated our method on the test set of the COCO dataset, and obtained 46 PQ. We hope that our approach will inspire the exploration of fully uniﬁed models for panoptic segmentation in future work.\n",
            "\n",
            "End-to-End Object Detection with Transformers 5 Conclusion We presented DETR, a new design for object detection systems based on transformers and bipartite matching loss for direct set prediction. The approach achieves comparable results to an optimized Faster R-CNN baseline on the challenging COCO dataset. DETR is straightforward to implement and has a ﬂexible architecture that is easily extensible to panoptic segmentation, with competitive results. In addition, it achieves signiﬁcantly better performance on large objects than Faster R-CNN, likely thanks to the processing of global information performed by the self-attention.\n",
            "\n",
            "This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects. Current detectors required several years of improvements to cope with similar issues, and we expect future work to successfully address them for DETR.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2caRN9Pot2fW"
      },
      "source": [
        "from transformers import pipeline, BartTokenizer\n",
        "\n",
        "# max seq length for this model = 1024\n",
        "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHd6OHMu1wex",
        "outputId": "bbac6d79-2f93-466a-e660-7547bfe8d19a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF4oME_7zLdx"
      },
      "source": [
        "def generate_chunks(new_paragraphs):\n",
        "  large_paragraph = \"\"\n",
        "  modified_paragraphs = []\n",
        "  for paragraph in new_paragraphs:\n",
        "    if len(tokenizer.encode(paragraph)) > 1024:\n",
        "      sents = nltk.sent_tokenize(paragraph)\n",
        "      small_paragraph = \"\"\n",
        "      paragraphs = []\n",
        "      for sent in sents:\n",
        "        if len(tokenizer.encode(small_paragraph))+len(tokenizer.encode(sent)) < 1024:\n",
        "          small_paragraph += ' '+sent\n",
        "        else:\n",
        "          paragraphs.append(small_paragraph)\n",
        "          small_paragraph = \"\"\n",
        "      modified_paragraphs.extend(paragraphs)\n",
        "    else:\n",
        "      if len(tokenizer.encode(large_paragraph)) + len(tokenizer.encode(paragraph))+1 < 1024:\n",
        "        large_paragraph += ' '+paragraph\n",
        "      else:\n",
        "        modified_paragraphs.append(large_paragraph)\n",
        "        # print(len(large_paragraph))\n",
        "        large_paragraph = paragraph"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5rp7hzL1TKy",
        "outputId": "914b4fa4-cf0a-4be4-fd8c-c90c1f413fe8"
      },
      "source": [
        "for p in modified_paragraphs:\n",
        "  print(len(p))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4128\n",
            "4959\n",
            "3508\n",
            "3641\n",
            "3392\n",
            "4491\n",
            "3643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJywem_IlJ58",
        "outputId": "5396c490-77bd-47a6-ee8b-9c0036eeb2ef"
      },
      "source": [
        "def generate_summarization(modified_paragpraphs):\n",
        "  summarization = []\n",
        "\n",
        "  for paragraph in modified_paragraphs:\n",
        "      summarized = summarizer(paragraph)\n",
        "      summarization.append(summarized[0]['summary_text'])\n",
        "\n",
        "  return '\\n'.join(summarization)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " object detection is a direct set prediction problem. We adopt an encoder-decoder architecture based on transformers. We train end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. We evaluate DETR on one of the most popular object detection datasets, COCO.\n",
            "There is no canonical deep learning model to directly predict sets. We follow the bipartite matching loss approach. We combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction. We describe our architecture in detail in Figure 2.\n",
            "DETR infers a ﬁxed-size set of N predictions, in a single pass through the decoder. The loss produces an optimal bipartite matching between predicted and ground truth objects. This optimal assignment is computed eﬃciently with the Hungarian algorithm. The matching cost takes into account both the class prediction and the similarity of predicted andGround Truth boxes.\n",
            "DETR achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO. We provide a detailed ablation study of the architecture and loss, with insights and qualitative results. We present results on panoptic segmentation, training only a small extension on a DETR model.\n",
            "We train DETR with AdamW setting the initial transformer’s learning rate to 10−4. The backbone is with ImageNet-pretrained ResNet model from torchvision with frozen batchnorm layers. We also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the ﬁrst convolution of this stage. This modiﬁcation increases the resolution by a factor of two.\n",
            "Attention mechanisms in the transformer decoder are the key components which model relations between feature representations of different detections. In our ablation analysis, we explore how other components of our architecture and loss inﬂuence the ﬁnal performance. For the study we choose ResNet-50-based DETR model with 6 encoder, 6 decoder layers and width 256.\n",
            "DETR learns different specialization for each query slot. We observe that each slot has several modes of operation focusing on different areas and box sizes. We hypothesize that this is related to the distribution of objects in COCO. We create a synthetic image3 to verify the generalization ability of DETR . Our model is able to ﬁnd all 24 giraffes on the image which is clearly out of distribution.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYTdIcDqmGmb"
      },
      "source": [
        "paragraphs = preprocess_text('filename')\n",
        "chunks = generate_chunks(paragraphs)\n",
        "summary = generate_summarization(chunks)\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}